{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28218,"status":"ok","timestamp":1654393498050,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"V58rxea0HqSa","outputId":"ab74f260-bc5b-4f94-9ac9-6b061eb4806c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connecting to security.ub\r0% [1 InRelease gpgv 1,581 B] [Connecting to archive.ubuntu.com (91.189.91.39)]\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,286 kB]\n","Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,992 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,231 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,512 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,799 kB]\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,021 kB]\n","Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Fetched 13.2 MB in 4s (3,007 kB/s)\n","Reading package lists... Done\n"]}],"source":["import os\n","# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.3'\n","spark_version = 'spark-3.0.3'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1654393498677,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"_xKwTpATHqSe","outputId":"850d01db-85ac-43c6-92a5-a6490aa7e232"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-06-05 01:44:57--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n","Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n","Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1002883 (979K) [application/java-archive]\n","Saving to: ‘postgresql-42.2.16.jar’\n","\n","postgresql-42.2.16. 100%[===================>] 979.38K  5.98MB/s    in 0.2s    \n","\n","2022-06-05 01:44:58 (5.98 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n","\n"]}],"source":["# Download the Postgres driver that will allow Spark to interact with Postgres.\n","!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMqDAjVS0KN9"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"M16-Amazon-Challenge\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"cyBsySGuY-9V"},"source":["### Load Amazon Data into Spark DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121561,"status":"ok","timestamp":1654393813190,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"CtCmBhQJY-9Z","outputId":"f07e44d2-ec6e-4194-e858-644cd01601a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n","+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","|         US|   27288431| R33UPQQUZQEM8|B005T4ND06|     400024643|Yoga for Movement...|       Video DVD|          5|            3|          3|   N|                Y|This was a gift f...|This was a gift f...| 2015-08-31|\n","|         US|   13722556|R3IKTNQQPD9662|B004EPZ070|     685335564|  Something Borrowed|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars| Teats my heart out.| 2015-08-31|\n","|         US|   20381037|R3U27V5QMCP27T|B005S9EKCW|     922008804|Les Miserables (2...|       Video DVD|          5|            1|          1|   N|                Y|        Great movie!|        Great movie.| 2015-08-31|\n","|         US|   24852644|R2TOH2QKNK4IOC|B00FC1ZCB4|     326560548|Alien Anthology a...|       Video DVD|          5|            0|          1|   N|                Y|             Amazing|My husband was so...| 2015-08-31|\n","|         US|   15556113|R2XQG5NJ59UFMY|B002ZG98Z0|     637495038|  Sex and the City 2|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|   Love this series.| 2015-08-31|\n","|         US|    6132474|R1N1KHBRR4ZTX3|B00X8RONBO|     896602391|When Calls The He...|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|            love it!| 2015-08-31|\n","|         US|   48049524|R3OM9S0TCBP38K|B000CEXFZG|     115883890|Teen Titans - The...|       Video DVD|          5|            0|          0|   N|                Y|               Great|Better than the c...| 2015-08-31|\n","|         US|    3282516|R1W4S949ZRCTBW|B00ID8H8EW|     977932459|      Generation War|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|I really emjoyed ...| 2015-08-31|\n","|         US|   51771179|R18JL1NNQAZFV2|B000TGJ8IU|     840084782|Troy  (Director's...|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|Awesome movie, we...| 2015-08-31|\n","|         US|   31816501|R1LP6PR06OPYUX|B00DPMPTDS|     262144920|Faith Aka the Gre...|       Video DVD|          4|            0|          0|   N|                Y|a beautiful fanta...|...a beautiful fa...| 2015-08-31|\n","|         US|   16164990| RZKBT035JA0UQ|B00X797LUS|     883589001|   Revenge: Season 4|       Video DVD|          5|            1|          2|   N|                Y|  It's a hit with me|I don't usually w...| 2015-08-31|\n","|         US|   33386989|R253N5W74SM7N3|B00C6MXB42|     734735137|YOUNG INDIANA JON...|       Video DVD|          4|            1|          1|   N|                Y|great stuff. I th...|great stuff.  I t...| 2015-08-31|\n","|         US|   45486371|R2D5IFTFPHD3RN|B000EZ9084|     821764517|     Survival Island|       Video DVD|          4|            1|          1|   N|                Y|          Four Stars|           very good| 2015-08-31|\n","|         US|   14006420|R1CECK3H1URK1G|B000CEXFZG|     115883890|Teen Titans - The...|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|Kids love the DVD...| 2015-08-31|\n","|         US|   23411619|R11MHQRE45204T|B00KXEM6XM|     651533797|     Fargo: Season 1|       Video DVD|          5|            0|          0|   N|                Y|A wonderful cover...|Great news Fargo ...| 2015-08-31|\n","|         US|    3202332| RSX9F068J0GXJ|B0036K9CM8|     231498541|Disney Parks: The...|       Video DVD|          5|            0|          0|   N|                Y|We love Disney an...|We love Disney an...| 2015-08-31|\n","|         US|   32012808|R1TUMFHZBBOWKL|B00G3HOJZ6|     847624020|Young Detective D...|       Video DVD|          5|            0|          0|   N|                Y|Excellent Movie f...|This is a surpris...| 2015-08-31|\n","|         US|   36507765| R8IZ1G1TWGK0E|B00V5E7YR2|     478422254|The Divergent Ser...|       Video DVD|          5|            0|          0|   N|                Y|          Five Stars|          Good movie| 2015-08-31|\n","|         US|   13696097|R1CBN0585B7BI4|B005BUA1JY|     700104332|Barney Miller: Th...|       Video DVD|          5|            0|          1|   N|                Y|          Five Stars|This show is a cl...| 2015-08-31|\n","|         US|    2190805|R2PJOAZ9I3D8O8|B00YCY46VO|     710263340|Inside Out (Blu-r...|       Video DVD|          5|            2|          2|   N|                N|An Instant Master...|Inside Out revolv...| 2015-08-31|\n","+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark import SparkFiles\n","url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_DVD_v1_00.tsv.gz\"\n","spark.sparkContext.addFile(url)\n","df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(\"\"), sep=\"\\t\", header=True, inferSchema=True)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"2yUSe55VY-9t"},"source":["### Create DataFrames to match tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8REmY1aY-9u"},"outputs":[],"source":["from pyspark.sql.functions import to_date\n","# Read in the Review dataset as a DataFrame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45621,"status":"ok","timestamp":1654394380196,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"B0TESUDRY-90","outputId":"0569dfee-35c5-42c5-9571-e4ab66812789"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+--------------+\n","|customer_id|customer_count|\n","+-----------+--------------+\n","|   24423656|             8|\n","|   24297214|             1|\n","|   12879980|             3|\n","|     515450|             2|\n","|   13313689|             1|\n","|   15523729|             4|\n","|    1673863|             1|\n","|   14552054|             2|\n","|   45392827|            14|\n","|   44178035|             1|\n","|   10522786|            10|\n","|   44848424|             3|\n","|   14230926|             1|\n","|   49243158|           274|\n","|   49084939|            10|\n","|   28777148|            41|\n","|   41836864|             1|\n","|    5219946|             1|\n","|   37795150|             1|\n","|   52081222|             1|\n","+-----------+--------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the customers_table DataFrame\n","customers_df = df.groupby(\"customer_id\").agg({\"customer_id\":\"count\"}).withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n","customers_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42666,"status":"ok","timestamp":1654394832490,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"4FwXA6UvY-96","outputId":"4ec259d1-ab1b-462c-f81f-6f7bee6c5b8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------------------+\n","|product_id|       product_title|\n","+----------+--------------------+\n","|B001DE29SS|Chaplin (15th Ann...|\n","|B00ZJ2GL6G|Jurassic World 3D...|\n","|B00T5DM876|Pretty Little Lia...|\n","|B004VW4V76|         Lebanon, PA|\n","|B009D4SFEC|Murdoch Mysteries...|\n","|B000GG4Y5K|Numb3rs - The Com...|\n","|B00383XZP8|TCM Spotlight: Ch...|\n","|B00I9MS86O|Breaking Bad: The...|\n","|B00005K3O4|Killer Klowns Fro...|\n","|B00BD6KETC|     Psych: Season 7|\n","|B00WHZZBTQ|Heaven Adores You...|\n","|B000092Q5C|Remo Williams - T...|\n","|B004AXPWH4|10 MINUTESOLUTION...|\n","|B00FEP9PQG|Boardwalk Empire:...|\n","|B0002F6BJW|Fractured Flicker...|\n","|B0007LXP7W|The A-Team - Seas...|\n","|B005ET4NHI|  Where The Boys Are|\n","|B005EBOSO4|Muscle Energy Tec...|\n","|B003V5CTKU|Black Label Socie...|\n","|B000087F6L|      Fahrenheit 451|\n","+----------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the products_table DataFrame and drop duplicates. \n","products_df = df.select([\"product_id\",\"product_title\"]).drop_duplicates()\n","products_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1654394917515,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"MkqyCuNQY-9-","outputId":"097117f0-017a-442b-e30d-d5b7c26ef138"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------+----------+--------------+-----------+\n","|     review_id|customer_id|product_id|product_parent|review_date|\n","+--------------+-----------+----------+--------------+-----------+\n","| R33UPQQUZQEM8|   27288431|B005T4ND06|     400024643| 2015-08-31|\n","|R3IKTNQQPD9662|   13722556|B004EPZ070|     685335564| 2015-08-31|\n","|R3U27V5QMCP27T|   20381037|B005S9EKCW|     922008804| 2015-08-31|\n","|R2TOH2QKNK4IOC|   24852644|B00FC1ZCB4|     326560548| 2015-08-31|\n","|R2XQG5NJ59UFMY|   15556113|B002ZG98Z0|     637495038| 2015-08-31|\n","|R1N1KHBRR4ZTX3|    6132474|B00X8RONBO|     896602391| 2015-08-31|\n","|R3OM9S0TCBP38K|   48049524|B000CEXFZG|     115883890| 2015-08-31|\n","|R1W4S949ZRCTBW|    3282516|B00ID8H8EW|     977932459| 2015-08-31|\n","|R18JL1NNQAZFV2|   51771179|B000TGJ8IU|     840084782| 2015-08-31|\n","|R1LP6PR06OPYUX|   31816501|B00DPMPTDS|     262144920| 2015-08-31|\n","| RZKBT035JA0UQ|   16164990|B00X797LUS|     883589001| 2015-08-31|\n","|R253N5W74SM7N3|   33386989|B00C6MXB42|     734735137| 2015-08-31|\n","|R2D5IFTFPHD3RN|   45486371|B000EZ9084|     821764517| 2015-08-31|\n","|R1CECK3H1URK1G|   14006420|B000CEXFZG|     115883890| 2015-08-31|\n","|R11MHQRE45204T|   23411619|B00KXEM6XM|     651533797| 2015-08-31|\n","| RSX9F068J0GXJ|    3202332|B0036K9CM8|     231498541| 2015-08-31|\n","|R1TUMFHZBBOWKL|   32012808|B00G3HOJZ6|     847624020| 2015-08-31|\n","| R8IZ1G1TWGK0E|   36507765|B00V5E7YR2|     478422254| 2015-08-31|\n","|R1CBN0585B7BI4|   13696097|B005BUA1JY|     700104332| 2015-08-31|\n","|R2PJOAZ9I3D8O8|    2190805|B00YCY46VO|     710263340| 2015-08-31|\n","+--------------+-----------+----------+--------------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the review_id_table DataFrame. \n","# Convert the 'review_date' column to a date datatype with to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n","review_id_df = df.select([\"review_id\",\"customer_id\",\"product_id\",\"product_parent\",to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")])\n","\n","# review_id_df = df.select([, to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")])\n","review_id_df.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1654394958589,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"lzMmkdKmY--D","outputId":"f7c0885b-cab5-42d8-f02a-b37b5fc2c64e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+-----------+-------------+-----------+----+-----------------+\n","|     review_id|star_rating|helpful_votes|total_votes|vine|verified_purchase|\n","+--------------+-----------+-------------+-----------+----+-----------------+\n","| R33UPQQUZQEM8|          5|            3|          3|   N|                Y|\n","|R3IKTNQQPD9662|          5|            0|          0|   N|                Y|\n","|R3U27V5QMCP27T|          5|            1|          1|   N|                Y|\n","|R2TOH2QKNK4IOC|          5|            0|          1|   N|                Y|\n","|R2XQG5NJ59UFMY|          5|            0|          0|   N|                Y|\n","|R1N1KHBRR4ZTX3|          5|            0|          0|   N|                Y|\n","|R3OM9S0TCBP38K|          5|            0|          0|   N|                Y|\n","|R1W4S949ZRCTBW|          5|            0|          0|   N|                Y|\n","|R18JL1NNQAZFV2|          5|            0|          0|   N|                Y|\n","|R1LP6PR06OPYUX|          4|            0|          0|   N|                Y|\n","| RZKBT035JA0UQ|          5|            1|          2|   N|                Y|\n","|R253N5W74SM7N3|          4|            1|          1|   N|                Y|\n","|R2D5IFTFPHD3RN|          4|            1|          1|   N|                Y|\n","|R1CECK3H1URK1G|          5|            0|          0|   N|                Y|\n","|R11MHQRE45204T|          5|            0|          0|   N|                Y|\n","| RSX9F068J0GXJ|          5|            0|          0|   N|                Y|\n","|R1TUMFHZBBOWKL|          5|            0|          0|   N|                Y|\n","| R8IZ1G1TWGK0E|          5|            0|          0|   N|                Y|\n","|R1CBN0585B7BI4|          5|            0|          1|   N|                Y|\n","|R2PJOAZ9I3D8O8|          5|            2|          2|   N|                N|\n","+--------------+-----------+-------------+-----------+----+-----------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the vine_table. DataFrame\n","# vine_df = df.select([])\n","vine_df = df.select([\"review_id\",\"star_rating\",\"helpful_votes\",\"total_votes\",\"vine\",\"verified_purchase\"])\n","vine_df.show()"]},{"cell_type":"markdown","metadata":{"id":"jITZhLkmY--J"},"source":["### Connect to the AWS RDS instance and write each DataFrame to its table. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jiUvs1aY--L"},"outputs":[],"source":["# Configure settings for RDS\n","mode = \"append\"\n","jdbc_url=\"jdbc:postgresql://database-2.cmzet40kog9n.us-west-1.rds.amazonaws.com:5432/postgres\"\n","config = {\"user\":\"postgres\", \n","          \"password\": \"Asmaisra2\", \n","          \"driver\":\"org.postgresql.Driver\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T2zgZ-aKY--Q"},"outputs":[],"source":["# Write review_id_df to table in RDS\n","review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":75081,"status":"error","timestamp":1654397953969,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"1m3yzn-LY--U","outputId":"edd0594b-1008-4c5e-cbfa-9cdc970a0c50"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-f9bb330a0bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write products_df to table in RDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# about 3 min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mproducts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'products_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o108.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 121 in stage 11.0 failed 1 times, most recent failure: Lost task 121.0 in stage 11.0 (TID 132, 5b44c517a1ad, executor driver): java.sql.BatchUpdateException: Batch entry 419 INSERT INTO products_table (\"product_id\",\"product_title\") VALUES ('B0001W9XOG','Life is a Dream: A Street Poet in New-York\tVideo DVD\t5\t1\t1\tN\tN\tfinding one''s purpose in life\tA poignant, powerfully made movie about a elderly street poet in New York City. Deeply eccentric but also filled with wisdom from his difficult life experiences, he brings meaning to his own life and the lives of those he touches by sharing his poetry with Central Park passerbys. I recommend this film highly; you will be truly moved and inspired by it!\t2005-05-21') was aborted: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B0001W9XOG) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:521)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1639)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:704)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B0001W9XOG) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:994)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:992)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:869)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:790)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 419 INSERT INTO products_table (\"product_id\",\"product_title\") VALUES ('B0001W9XOG','Life is a Dream: A Street Poet in New-York\tVideo DVD\t5\t1\t1\tN\tN\tfinding one''s purpose in life\tA poignant, powerfully made movie about a elderly street poet in New York City. Deeply eccentric but also filled with wisdom from his difficult life experiences, he brings meaning to his own life and the lives of those he touches by sharing his poetry with Central Park passerbys. I recommend this film highly; you will be truly moved and inspired by it!\t2005-05-21') was aborted: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B0001W9XOG) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:521)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1639)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:704)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_table_pkey\"\n  Detail: Key (product_id)=(B0001W9XOG) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n\t... 18 more\n"]}],"source":["# Write products_df to table in RDS\n","# about 3 min\n","products_df.write.jdbc(url=jdbc_url, table='products_table', mode=mode, properties=config)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":175892,"status":"ok","timestamp":1654399839752,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"KbXri15fY--Z"},"outputs":[],"source":["# Write customers_df to table in RDS\n","# 5 min 14 s\n","customers_df.write.jdbc(url=jdbc_url, table='customers_table', mode=mode, properties=config)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":503161,"status":"ok","timestamp":1654400380250,"user":{"displayName":"Tahani Sury","userId":"02529814967976047171"},"user_tz":420},"id":"XdQknSHLY--e"},"outputs":[],"source":["# Write vine_df to table in RDS\n","# 11 minutes\n","vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Exuo6ebUsCqW"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Amazon_Reviews_ETL.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"nteract":{"version":"0.12.3"}},"nbformat":4,"nbformat_minor":0}